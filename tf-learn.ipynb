{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New job: Mid-level job\n",
      "Processing job:New job: Processing job: Important job Low-level job\n",
      "Mid-level job\n",
      "Processing job:\n",
      "New job:  Low-level jobImportant job\n",
      "\n",
      "(<tf.Tensor 'Const:0' shape=() dtype=float32>, <tf.Tensor 'Const_1:0' shape=() dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('node3:', <tf.Tensor 'Add:0' shape=() dtype=float32>)\n",
      "('sess.run(node3):', 7.0)\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3:\", node3)\n",
    "print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n",
      "22.5\n",
      "23.66\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b\n",
    "print(sess.run(adder_node, {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))\n",
    "add_and_triple = adder_node * 3\n",
    "print(sess.run(add_and_triple, {a: 3, b: 4.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30000001]\n",
      "[-0.30000001]\n",
      "[ 0.          0.30000001  0.60000002  0.90000004]\n",
      "23.66\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print(sess.run(W))\n",
    "print(sess.run(b))\n",
    "print(sess.run(linear_model, {x: [1, 2, 3, 4]}))\n",
    "\n",
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))\n",
    "\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y:[0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n",
      "5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "\n",
    "print(sess.run([W, b]))\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y:[0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpNNe5IT\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f653a448650>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpNNe5IT', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpNNe5IT/model.ckpt.\n",
      "INFO:tensorflow:loss = 27.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1104.87\n",
      "INFO:tensorflow:loss = 0.438823, step = 101 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1214.46\n",
      "INFO:tensorflow:loss = 0.0630407, step = 201 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1245.72\n",
      "INFO:tensorflow:loss = 0.0606548, step = 301 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1225.73\n",
      "INFO:tensorflow:loss = 0.0137, step = 401 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1355.87\n",
      "INFO:tensorflow:loss = 0.003198, step = 501 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1298.28\n",
      "INFO:tensorflow:loss = 0.00225023, step = 601 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1375.84\n",
      "INFO:tensorflow:loss = 0.000521875, step = 701 (0.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 1353.35\n",
      "INFO:tensorflow:loss = 0.000195021, step = 801 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 1186.66\n",
      "INFO:tensorflow:loss = 6.37646e-05, step = 901 (0.086 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpNNe5IT/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.2245e-05.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-28-03:44:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpNNe5IT/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-28-03:44:07\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 3.00629e-06, global_step = 1000, loss = 1.20252e-05\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-28-03:44:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpNNe5IT/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-28-03:44:08\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.00267411, global_step = 1000, loss = 0.0106964\n",
      "train metrics: {'average_loss': 3.0062904e-06, 'global_step': 1000, 'loss': 1.2025162e-05}\n",
      "eval metrics: {'average_loss': 0.0026741084, 'global_step': 1000, 'loss': 0.010696433}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpLIeSPQ\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f653a32c850>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpLIeSPQ', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpLIeSPQ/model.ckpt.\n",
      "INFO:tensorflow:loss = 14.2240074054, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1137.15\n",
      "INFO:tensorflow:loss = 0.200969409148, step = 101 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1121.96\n",
      "INFO:tensorflow:loss = 0.0201700701431, step = 201 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 1170.65\n",
      "INFO:tensorflow:loss = 0.00171234966107, step = 301 (0.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 1402.56\n",
      "INFO:tensorflow:loss = 0.00025203215403, step = 401 (0.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 1500.2\n",
      "INFO:tensorflow:loss = 1.54261325921e-05, step = 501 (0.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 1394.04\n",
      "INFO:tensorflow:loss = 1.16356222056e-06, step = 601 (0.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 1491.67\n",
      "INFO:tensorflow:loss = 1.49196549776e-07, step = 701 (0.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 1441.26\n",
      "INFO:tensorflow:loss = 1.46021089523e-08, step = 801 (0.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 1668.7\n",
      "INFO:tensorflow:loss = 1.22070256137e-09, step = 901 (0.060 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpLIeSPQ/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 7.31555273118e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-28-03:54:00\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpLIeSPQ/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-28-03:54:00\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 1.15569e-10\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-28-03:54:00\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpLIeSPQ/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-28-03:54:01\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.010102\n",
      "train metrics: {'loss': 1.1556932e-10, 'global_step': 1000}\n",
      "eval metrics: {'loss': 0.010101988, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 声明特征列表，我们只有一个实数特征\n",
    "def model_fn(features, labels, mode):\n",
    "  # 构建一个线性模型并预测值\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W * features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # EstimatorSpec connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# train\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x,W)+ b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices = [1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict = {x: mnist.test.images,y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#####针对专家的深度MNIST############\n",
    "##################################\n",
    "\n",
    "#权重初始化\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积和池化\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "#第一卷积层\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#第二卷积层\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#密集层\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#Readout层\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.04\n",
      "step 100, training accuracy 0.82\n",
      "step 200, training accuracy 0.88\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 0.96\n",
      "step 500, training accuracy 0.9\n",
      "step 600, training accuracy 0.94\n",
      "step 700, training accuracy 1\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.92\n",
      "step 1000, training accuracy 0.92\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 0.96\n",
      "step 1300, training accuracy 1\n",
      "step 1400, training accuracy 0.96\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 1\n",
      "step 1800, training accuracy 0.96\n",
      "step 1900, training accuracy 0.96\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 0.96\n",
      "step 2200, training accuracy 0.92\n",
      "step 2300, training accuracy 1\n",
      "step 2400, training accuracy 0.98\n",
      "step 2500, training accuracy 1\n",
      "step 2600, training accuracy 0.94\n",
      "step 2700, training accuracy 0.96\n",
      "step 2800, training accuracy 1\n",
      "step 2900, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3100, training accuracy 1\n",
      "step 3200, training accuracy 0.98\n",
      "step 3300, training accuracy 0.98\n",
      "step 3400, training accuracy 0.98\n",
      "step 3500, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3700, training accuracy 1\n",
      "step 3800, training accuracy 0.96\n",
      "step 3900, training accuracy 1\n",
      "step 4000, training accuracy 0.98\n",
      "step 4100, training accuracy 0.98\n",
      "step 4200, training accuracy 1\n",
      "step 4300, training accuracy 0.98\n",
      "step 4400, training accuracy 1\n",
      "step 4500, training accuracy 1\n",
      "step 4600, training accuracy 0.96\n",
      "step 4700, training accuracy 0.98\n",
      "step 4800, training accuracy 1\n",
      "step 4900, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5100, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5300, training accuracy 1\n",
      "step 5400, training accuracy 0.98\n",
      "step 5500, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5700, training accuracy 1\n",
      "step 5800, training accuracy 0.96\n",
      "step 5900, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6100, training accuracy 0.98\n",
      "step 6200, training accuracy 1\n",
      "step 6300, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6500, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6700, training accuracy 0.98\n",
      "step 6800, training accuracy 1\n",
      "step 6900, training accuracy 0.98\n",
      "step 7000, training accuracy 0.98\n",
      "step 7100, training accuracy 0.98\n",
      "step 7200, training accuracy 1\n",
      "step 7300, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7500, training accuracy 0.98\n",
      "step 7600, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "#训练并评估模型\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "          train_accuracy = accuracy.eval(feed_dict={\n",
    "              x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "          print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "  x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6522b60e10>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/iris_model', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-2000\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 11.4432, step = 2001\n",
      "INFO:tensorflow:global_step/sec: 961.447\n",
      "INFO:tensorflow:loss = 2.91436, step = 2101 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 1019.22\n",
      "INFO:tensorflow:loss = 5.59994, step = 2201 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 915.064\n",
      "INFO:tensorflow:loss = 6.02238, step = 2301 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 925.164\n",
      "INFO:tensorflow:loss = 2.17796, step = 2401 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 947.319\n",
      "INFO:tensorflow:loss = 6.2717, step = 2501 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 1020.65\n",
      "INFO:tensorflow:loss = 1.63681, step = 2601 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 956.736\n",
      "INFO:tensorflow:loss = 6.20328, step = 2701 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 972.394\n",
      "INFO:tensorflow:loss = 4.78532, step = 2801 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 1056.48\n",
      "INFO:tensorflow:loss = 2.07688, step = 2901 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 957.287\n",
      "INFO:tensorflow:loss = 4.50716, step = 3001 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 1115.28\n",
      "INFO:tensorflow:loss = 7.08771, step = 3101 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1094.07\n",
      "INFO:tensorflow:loss = 2.29207, step = 3201 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1044.48\n",
      "INFO:tensorflow:loss = 4.36272, step = 3301 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 986.493\n",
      "INFO:tensorflow:loss = 10.327, step = 3401 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 924.326\n",
      "INFO:tensorflow:loss = 3.97546, step = 3501 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 846.662\n",
      "INFO:tensorflow:loss = 5.19642, step = 3601 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.59\n",
      "INFO:tensorflow:loss = 4.47641, step = 3701 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 1063.33\n",
      "INFO:tensorflow:loss = 4.78981, step = 3801 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1018.82\n",
      "INFO:tensorflow:loss = 4.64709, step = 3901 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 970.913\n",
      "INFO:tensorflow:loss = 1.18765, step = 4001 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 798.237\n",
      "INFO:tensorflow:loss = 1.1236, step = 4101 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 779.174\n",
      "INFO:tensorflow:loss = 2.47383, step = 4201 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 878.758\n",
      "INFO:tensorflow:loss = 4.78021, step = 4301 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 970.317\n",
      "INFO:tensorflow:loss = 6.51493, step = 4401 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 1022.88\n",
      "INFO:tensorflow:loss = 1.18918, step = 4501 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1063.29\n",
      "INFO:tensorflow:loss = 6.10424, step = 4601 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 925.164\n",
      "INFO:tensorflow:loss = 8.8561, step = 4701 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 723.076\n",
      "INFO:tensorflow:loss = 4.99773, step = 4801 (0.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 725.327\n",
      "INFO:tensorflow:loss = 6.18065, step = 4901 (0.139 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.92614.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-28-08:08:06\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-28-08:08:06\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.966667, average_loss = 0.0881405, global_step = 5000, loss = 2.64421\n",
      "\n",
      "Test Accuracy: 0.966667\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-5000\n",
      "New Samples, Class Predictions:    [array(['1'], dtype=object), array(['2'], dtype=object)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf.estimator Quickstart\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "\n",
    "def main():\n",
    "  # If the training and test sets aren't stored locally, download them.\n",
    "  if not os.path.exists(IRIS_TRAINING):\n",
    "    raw = urllib.urlopen(IRIS_TRAINING_URL).read()\n",
    "    with open(IRIS_TRAINING, \"w\") as f:\n",
    "        f.write(raw)\n",
    "\n",
    "  if not os.path.exists(IRIS_TEST):\n",
    "    raw = urllib.urlopen(IRIS_TEST_URL).read()\n",
    "    with open(IRIS_TEST, \"w\") as f:\n",
    "        f.write(raw)\n",
    "\n",
    "  # Load datasets.\n",
    "  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=IRIS_TRAINING,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "  test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=IRIS_TEST,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "\n",
    "  # Specify that all features have real-value data\n",
    "  feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\n",
    "\n",
    "  # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "  classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                          hidden_units=[10, 20, 10],\n",
    "                                          n_classes=3,\n",
    "                                          model_dir=\"/tmp/iris_model\")\n",
    "  # Define the training inputs\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(training_set.data)},\n",
    "      y=np.array(training_set.target),\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "  # Train model.\n",
    "  classifier.train(input_fn=train_input_fn, steps=3000)\n",
    "\n",
    "  # Define the test inputs\n",
    "  test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(test_set.data)},\n",
    "      y=np.array(test_set.target),\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "  # Evaluate accuracy.\n",
    "  accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "\n",
    "  print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n",
    "\n",
    "  # Classify two new flower samples.\n",
    "  new_samples = np.array(\n",
    "      [[6.4, 3.2, 4.5, 1.5],\n",
    "       [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "  predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": new_samples},\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "  predictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "  predicted_classes = [p[\"classes\"] for p in predictions]\n",
    "\n",
    "  print(\n",
    "      \"New Samples, Class Predictions:    {}\\n\"\n",
    "      .format(predicted_classes))\n",
    "    \n",
    "#The model thus predicts that the first sample is Iris versicolor, and the second sample is Iris virginica.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Pattern object at 0x7fbaa86cde70>112\n"
     ]
    }
   ],
   "source": [
    "#-*- encoding:utf-8 -*-\n",
    "import re\n",
    "f=open(r'/home/runner/Downloads/login.txt','r')\n",
    "login={}\n",
    "lines=f.readlines()\n",
    "for line in lines:\n",
    "    pid=re.compile(r'\\d{18}')\n",
    "    match=pid.match(line)\n",
    "    if match:\n",
    "        pid=match.group()\n",
    "    if(login.has_key(pid)):\n",
    "        login[pid]+=1\n",
    "    else:\n",
    "        login[pid]=1\n",
    "f.close()\n",
    "numlist=list(set(login.values()))\n",
    "numlist.sort(reverse=True)\n",
    "for pidnum in numlist:\n",
    "    for pid in login:\n",
    "        if(pidnum==login[pid]):\n",
    "            print str(pid)+str(login[pid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as its\n",
    "words = \"1234567890\"\n",
    "r = its.product(words,repeat=8)\n",
    "dic = open(\"/home/runner/dictionary.txt\",'a')\n",
    "for i in r:\n",
    "    dic.write(\"\".join(i)+\"\\n\")\n",
    "\n",
    "dic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- encoding:utf-8 -*-\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime,timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def get_current_total_num():\n",
    "    login_url = ''\n",
    "    data = {\n",
    "        'username':'',\n",
    "        'password':''\n",
    "        }\n",
    "    s = requests.session()\n",
    "    get_login = s.get(login_url)\n",
    "    post_login = s.post(login_url,data=data)\n",
    "    base_sql_url = ''\n",
    "    base_data = {'db_type':'ga_data',\n",
    "                 'search_sql':'select * from d_player',\n",
    "                 'page':'1'}\n",
    "#     s.get(base_sql_url)\n",
    "    base_result = s.post(base_sql_url,data = base_data)\n",
    "    base_html = base_result.text\n",
    "    print base_html\n",
    "a=get_current_total_num()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
